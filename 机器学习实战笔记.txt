《机器学习实战》
	利用计算机彰显数据背后的真实含义。
用于执行分类、回归、聚类和密度估计的机器学习算法：

监督学习的用途：
1、K-近邻算法；2、朴素贝叶斯算法；3、SVM；4、决策树；
5、线性回归；6、局部加权线性回归；7、Ridge回归；8、Lasso最小回归系数估计

无监督学习的用途：
1、K-均值；2、DBSCAN；3、EM（最大期望算法）4、Parzen窗设计

****************************************************************
**机器学习的过程：
**收集数据->准备输入数据->分析数据->训练算法->测试算法->使用算法
****************************************************************

分类：
1、K-近邻算法，使用距离矩阵分类；
2、决策树分类；
3、概率分类；
4、logistic回归分类：参数优化；
5、SVM；
6、AdaBoost

***K-近邻算法***
优点：精度高，对异常数据不敏感，无数据输入假定；
缺点：计算复杂度高，空间复杂度高；（是一种非持久化算法，及每次使用都要进行训练）
适用数据类型：数值型、标称型数据。
实现：
1）计算已知类别数据集中的点与当前点之间的距离；
2）按照距离递增次序排序；
3）选取与当前点距离最小的K个点；
4）确定前K个点所在类别的出现频率；
5）返回前K个点出现频率最高的类别作为当前点的预测分类。
NOTE:进行训练和测试的数据集都必须进行归一化，否则有较大差值的特征将会对测试结果有显著的影响（因为欧几里得距离对每个属性的权值是相同的）。
     同一数据集可进行拆解，一部分用于作为训练集，另一部分作为测试集。

*** 决策树 ***
优点：计算复杂度不高，数据结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据；
缺点：可能会产生过度匹配问题；
适用数据类型：数值型、标称型。
实现：
实现问题：1）找寻最佳划分特征；2）找寻特征划分的阈值；3）生成树终止条件
《ID3算法》----无法处理数值型数据，原因在于其使用信息增益选取最佳划分特征。
1）计算数据集的信息熵，为求熵增益做准备；
2）评估特征，（采用数据集的信息增益寻找划分数据集的最佳特征）
	信息熵越高，则混合的数据也越多（即数据集的无序程度或者种类）；
3）划分数据集。对每个特征划分数据集的结果计算一次信息熵增益，判断最好划分；
4）递归构建决策树。递归结束条件：遍历完所有划分数据集的属性，且在叶节点采用多数表决的方式进行类别划分；

*** 朴素贝叶斯 ***
朴素贝叶斯分类器通常有两种实现方式：一种基于伯努利模型实现，一种基于多项式模型实现。
此处采用前一种，该方式中并不考虑词在文档中出现的次数，只考虑出不出现，相当于假设词是等权重的。

<朴素假设：各特征之间相互独立，且同等重要>

优点：在数据较少的情况下，通过特征之间的独立性假设，仍然有效，可以处理多类别问题；
缺点：对于输入数据的准备方式较为敏感；
适用数据类型：标称型。
需考虑的问题：下溢出。词袋模型在解决文档分类问题比词集模型有所提高。可对文档切分器进行优化。


*** Logistic回归 ***
优点：计算代价不高，易于理解和实现。
缺点：容易出现欠拟合现象，分类精度可能不高。
适用数据类型：数值型、标称型

优化算法：梯度上升算法，随机梯度上升算法，牛顿法


*** 支持向量机 ***

优点：可以很好的用于高维数据，避免维灾难。
特点：使用训练实例的一个子集表示决策边界，该子集称作支持向量。

缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二分类问题。
适用数据类型：数值型、标称型

优化算法：SMO算法














