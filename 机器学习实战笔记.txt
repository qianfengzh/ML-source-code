《机器学习实战》
	利用计算机彰显数据背后的真实含义。
用于执行分类、回归、聚类和密度估计的机器学习算法：

监督学习的用途：
1、K-近邻算法；2、朴素贝叶斯算法；3、SVM；4、决策树；
5、线性回归；6、局部加权线性回归；7、Ridge回归；8、Lasso最小回归系数估计

无监督学习的用途：
1、K-均值；2、DBSCAN；3、EM（最大期望算法）4、Parzen窗设计

****************************************************************
**机器学习的过程：
**收集数据->准备输入数据->分析数据->训练算法->测试算法->使用算法
****************************************************************

分类：
1、K-近邻算法，使用距离矩阵分类；
2、决策树分类；
3、概率分类；
4、logistic回归分类：参数优化；
5、SVM；
6、AdaBoost

***K-近邻算法***
优点：精度高，对异常数据不敏感，无数据输入假定；
缺点：计算复杂度高，空间复杂度高；（是一种非持久化算法，及每次使用都要进行训练）
适用数据类型：数值型、标称型数据。
实现：
1）计算已知类别数据集中的点与当前点之间的距离；
2）按照距离递增次序排序；
3）选取与当前点距离最小的K个点；
4）确定前K个点所在类别的出现频率；
5）返回前K个点出现频率最高的类别作为当前点的预测分类。
NOTE:进行训练和测试的数据集都必须进行归一化，否则有较大差值的特征将会对测试结果有显著的影响（因为欧几里得距离对每个属性的权值是相同的）。
     同一数据集可进行拆解，一部分用于作为训练集，另一部分作为测试集。

*** 决策树 ***
是一种贪心算法，并不考虑能否达到全局最优。
优点：计算复杂度不高，数据结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据；
缺点：可能会产生过度匹配问题；
适用数据类型：数值型、标称型。
实现：
实现问题：1）找寻最佳划分特征；2）找寻特征划分的阈值；3）生成树终止条件
《ID3算法》----无法处理数值型数据，原因在于其使用信息增益选取最佳划分特征。
1）计算数据集的信息熵，为求熵增益做准备；
2）评估特征，（采用数据集的信息增益寻找划分数据集的最佳特征）
	信息熵越高，则混合的数据也越多（即数据集的无序程度或者种类）；
3）划分数据集。对每个特征划分数据集的结果计算一次信息熵增益，判断最好划分；
4）递归构建决策树。递归结束条件：遍历完所有划分数据集的属性，且在叶节点采用多数表决的方式进行类别划分；

*** 朴素贝叶斯 ***
朴素贝叶斯分类器通常有两种实现方式：一种基于伯努利模型实现，一种基于多项式模型实现。
此处采用前一种，该方式中并不考虑词在文档中出现的次数，只考虑出不出现，相当于假设词是等权重的。

<朴素假设：各特征之间相互独立，且同等重要>

优点：在数据较少的情况下，通过特征之间的独立性假设，仍然有效，可以处理多类别问题；
缺点：对于输入数据的准备方式较为敏感；
适用数据类型：标称型。
需考虑的问题：下溢出。词袋模型在解决文档分类问题比词集模型有所提高。可对文档切分器进行优化。


*** Logistic回归 ***
优点：计算代价不高，易于理解和实现。
缺点：容易出现欠拟合现象，分类精度可能不高。
适用数据类型：数值型、标称型

优化算法：梯度上升算法，随机梯度上升算法，牛顿法


*** 支持向量机 ***

优点：可以很好的用于高维数据，避免维灾难。
特点：使用训练实例的一个子集表示决策边界，该子集称作支持向量。

缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二分类问题。
适用数据类型：数值型、标称型

优化算法：SMO算法




*** AdBoost元 ***




*** 线性回归 ***  目的：预测
（线性回归、局部加权线性回归、岭回归和逐步线性回归）

优点：结果易于理解，计算上不复杂；
缺点：对非线性的数据拟合不好；
适用数据类型：数值型、标称型

测试算法时，可用原训练集进行预测，在预测值和真实值之间求相关西湖，可验证预测值同真实值的匹配程度。



*** 回归树 ***

优点：可以对复杂和非线性的数据建模；
缺点：结果不易理解；
适用数据类型：数值型、标称型。
回归树与分类树实现的思路相似，但叶节点的数据类型不是离散型，而是连续型。

回归树的实现特点：
    1）回归树在叶节点判断上使用的是目标变量的均值，而决策树中使用的是数量多的类；
    2）回归树在数据离散度判断（判断最佳分类特征和特征值）上使用的是平方误差和函数，而决策树中使用的是信息增益率。
回归树在构建时需要树剪枝，通常分为预剪枝和后剪枝：
    预剪枝：（需用户指定参数）在构建树的过程中进行剪枝，如可在寻找最佳划分特征时进行剪枝，以减少判断的中间节点；
    后剪枝：先构建足够大、足够复杂的树，自上而下找到叶节点，用测试集判断合并叶节点是否能降低测试误差，如果能降低，就合并叶节点。
为寻求最佳的模型，可以同时使用两种剪枝技术。

-- 模型树 --（适用于数据集中有多个线性模型）
    用树对数据建模，除了把叶节点设为常数值外，还有种方法是把节点设定为分段线性函数。即模型由多个线性片段组成。
    模型树的可解释性是它优于回归树的特点之一，并且其有更高的预测准确度。

模型书、回归树、线性回归、局部加权线性回归、岭回归、逐步线性回归那个更好：
    客观方法：计算相关系数，通过 numpy 库的 corrcoef(yHat, y, rowvar=0)求解。

*****************************
**  模型性能对比分析
*****************************















*** 数据挖掘中的其他工具 ***
PCA(Principal Component Analysis)：按照数据方差最大方向调整数据的主成分分析降维（dimensionality reduction）；
SVD(Singular Value Decomposition)：一种矩阵分解技术，通过对原始数据的逼近来达到降维目的；

降维技术的原因：
1、使得数据集更易使用；
2、降低很多算法的计算开销；
3、去除噪声；
4、使得结果易懂。

降维技术：PCA、因子分析(Factor Analysis)、独立成分分析(Independent Component Analysis)

## PCA ##
数据从原来的坐标系转换到了新的坐标系。
    第一个新坐标轴选择的是原始数据中方差最大的方向；第二个新坐标轴的选择和第一个坐标轴正交且具有最大方差的方向。
该过程一直重复，重复次数为原始数据中特征的数目。会发现大部分方差都包含在最前面几个新坐标轴中，故可忽略余下的坐标轴。

优点：降低数据的复杂性，识别最重要的特征。
缺点：不一定需要，且可能损失有用的信息。
适用数据类型：数值型。




## SVD ##
    SVD是矩阵分解的一种类型，矩阵分解时将数据矩阵分解为多个独立部分的过程。

优点：简化数据，去除噪声，提高算法的结果；
缺点：数据的转换可能难以理解；
适用数据类型：数值型。
    我们称利用SVD的方法为隐性语义索引(Latent Semantic Indexing,LSI)或隐性语义分析(Latent Semantic Analysis,LSA)。

