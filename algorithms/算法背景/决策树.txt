决策树：
	决策树是对数据分类，以此达到预测的目的。由决策结点、分支和叶子组成。利用若干个变量来判断所属的类别。
需要明确的问题：
	1）属性选择度量（分裂规则）----比较流行的度量量：信息增益（熵）、增益率、Gini指标
	2）循环终止条件
	3）树剪枝

ID3算法：
	一种贪心算法，用于构建决策树。核心：（信息熵、信息增益）
信息熵理解：信息熵数学定义是负的log函数，故随着概率的增加信息量减小（即属性的值分类越多，其信息量越大）

特点：构造完全树，后剪枝法。
缺陷：
	1）在特征选取时，具有多值偏向性（取值较多的属性并不总是最重要的，如：姓名）；
	2）ID3算法不能处理具有连续值的属性，也不能处理具有缺失数据的属性；
	3）用互信息做属性选择标准，存在着一个假设，即训练子集中正、反例比例同实际问题中正、反例比例一直（很难保证，互信息计算存在偏差）；
	4）每个结点仅含一个属性，是一种单变元算法，使生成决策树结点的相关性不够强；
	5）内存占用率较大，耗费资源，需将整个数据集读入内存中。
一旦按某特征切分后，该特征在之后的算法中将不起作用，切分过于迅速。

数据要求：相关属性必须是离散化属性，或标称属性，且无缺失数据的属性。

原理：
以信息熵的下降速度为选取决策属性的标准，即在每个节点选取剩余未被用来划分的具有最高信息增益的属性最为决策标准，然后继续执行这个过程选取每个节点的决策属性，直到生成的决策树能完美的分类训练数据集。


C4.5算法：
（其并不是一个算法，而是一组算法，非剪枝C4.5和C4.5规则）
	用于构建决策树。ID3的改进算法。核心：（信息增益率）
改进：
	1）用信息增益率选择属性，克服用信息增益选择属性时偏向选择取值多的属性的不足；
	2）构造树的过程中进行剪枝；
	3）能够完成对连续属性的离散化处理；
	4）能够对不完整数据进行处理。
特点：悲观剪枝法，利用训练集剪枝（不需要独立的剪枝集）
优点：产生的分类规则易于理解，准确率较高。
缺点：
	1）在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法比较低效；
	2）此外，C4.5算法只适合于能够驻留于内存的数据集，当训练集无法在内存容纳时程序将无法运行。

实现：
	1）




CART算法：（实现回归树）
1、使用二元切分来处理连续变量，进而可以较好的处理连续特征；
2、最佳特征和最佳划分特征值使用目标值的平方误差和判断；
3、叶节点的判断使用的是目标值的均值。